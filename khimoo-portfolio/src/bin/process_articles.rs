use khimoo_portfolio::article_processing::{
    FrontMatterParser, ArticleMetadata, LinkExtractor, ExtractedLink
};
use anyhow::{Context, Result};
use clap::Parser;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use walkdir::WalkDir;
use chrono::Utc;

#[derive(Parser)]
#[command(name = "process-articles")]
#[command(about = "Process articles and generate static data")]
struct Args {
    /// Articles directory path
    #[arg(short, long, default_value = "articles")]
    articles_dir: PathBuf,
    
    /// Output directory for generated data
    #[arg(short, long, default_value = "data")]
    output_dir: PathBuf,
    
    /// Enable verbose logging
    #[arg(short, long)]
    verbose: bool,
    
    /// Enable parallel processing
    #[arg(short, long)]
    parallel: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProcessedArticle {
    pub slug: String,
    pub title: String,
    pub content: String,
    pub metadata: ArticleMetadata,
    pub file_path: String,
    pub outbound_links: Vec<ExtractedLink>,
    pub inbound_count: usize,

    pub processed_at: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ArticlesData {
    pub articles: Vec<ProcessedArticle>,
    pub generated_at: String,
    pub total_count: usize,
    pub home_articles: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LinkGraphData {
    pub graph: HashMap<String, GraphNode>,
    pub generated_at: String,
    pub total_connections: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GraphNode {
    pub connections: Vec<GraphConnection>,
    pub inbound_count: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GraphConnection {
    pub target: String,
    pub connection_type: ConnectionType,
    pub bidirectional: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ConnectionType {
    DirectLink,
    Bidirectional,
}

fn main() -> Result<()> {
    let args = Args::parse();
    
    if args.verbose {
        println!("üîÑ Processing articles from {:?}", args.articles_dir);
        println!("üìÅ Output directory: {:?}", args.output_dir);
        println!("‚ö° Parallel processing: {}", args.parallel);
    }
    
    let processor = ArticleProcessor::new(args.articles_dir, args.output_dir, args.verbose);
    
    if args.parallel {
        processor.process_all_articles_parallel()
    } else {
        processor.process_all_articles_sequential()
    }
}

pub struct ArticleProcessor {
    articles_dir: PathBuf,
    output_dir: PathBuf,
    verbose: bool,
    link_extractor: LinkExtractor,
}

impl ArticleProcessor {
    pub fn new(articles_dir: PathBuf, output_dir: PathBuf, verbose: bool) -> Self {
        Self {
            articles_dir,
            output_dir,
            verbose,
            link_extractor: LinkExtractor::new().expect("Failed to create LinkExtractor"),
        }
    }

    pub fn process_all_articles_sequential(&self) -> Result<()> {
        // Create output directory
        std::fs::create_dir_all(&self.output_dir)
            .context("Failed to create output directory")?;
        
        // Load and parse all articles
        let articles = self.load_and_parse_articles()
            .context("Failed to load articles")?;
        
        if self.verbose {
            println!("üìö Found {} articles", articles.len());
        }
        
        // Calculate inbound link counts
        let articles_with_counts = self.calculate_inbound_counts(articles)
            .context("Failed to calculate inbound counts")?;
        
        // Build link graph
        let link_graph = self.build_link_graph(&articles_with_counts)
            .context("Failed to build link graph")?;
        
        // Write output files
        self.write_articles_data(&articles_with_counts)
            .context("Failed to write articles data")?;
        self.write_link_graph_data(&link_graph)
            .context("Failed to write link graph data")?;
        
        println!("‚úÖ Successfully processed {} articles", articles_with_counts.len());
        
        // Display summary
        self.display_summary(&articles_with_counts, &link_graph);
        
        Ok(())
    }

    pub fn process_all_articles_parallel(&self) -> Result<()> {
        // For now, fall back to sequential processing
        // TODO: Implement parallel processing with rayon
        if self.verbose {
            println!("‚ö†Ô∏è  Parallel processing not yet implemented, using sequential");
        }
        self.process_all_articles_sequential()
    }

    fn load_and_parse_articles(&self) -> Result<Vec<ProcessedArticle>> {
        let mut articles = Vec::new();
        
        for entry in WalkDir::new(&self.articles_dir)
            .into_iter()
            .filter_map(|e| e.ok())
            .filter(|e| e.path().extension().map_or(false, |ext| ext == "md"))
        {
            match self.process_article_file(entry.path()) {
                Ok(article) => {
                    if self.verbose {
                        println!("‚úÖ Processed: {} - '{}'", 
                            entry.path().display(), 
                            article.title
                        );
                    }
                    articles.push(article);
                }
                Err(e) => {
                    eprintln!("‚ùå Error processing {}: {}", entry.path().display(), e);
                    return Err(e);
                }
            }
        }
        
        Ok(articles)
    }

    fn process_article_file(&self, file_path: &Path) -> Result<ProcessedArticle> {
        // Read file content
        let content = std::fs::read_to_string(file_path)
            .with_context(|| format!("Failed to read file: {:?}", file_path))?;

        // Parse front matter
        let (metadata, markdown_content) = FrontMatterParser::parse(&content)
            .with_context(|| format!("Failed to parse front matter in: {:?}", file_path))?;

        // Validate metadata
        FrontMatterParser::validate_metadata(&metadata)
            .with_context(|| format!("Invalid metadata in: {:?}", file_path))?;

        // Extract links from content
        let outbound_links = self.link_extractor.extract_links(&markdown_content);

        // Generate slug from file path
        let slug = self.generate_slug(file_path);

        if self.verbose {
            println!("   üìù Title: {}", metadata.title);
            println!("   üÜî Slug: {}", slug);
            println!("   üè† Home display: {}", metadata.home_display);
            if let Some(category) = &metadata.category {
                println!("   üìÇ Category: {}", category);
            }
            println!("   ‚≠ê Importance: {}", metadata.importance);
            if !outbound_links.is_empty() {
                println!("   üîó Outbound links: {}", outbound_links.len());
                for link in &outbound_links {
                    println!("      ‚Üí {} ({})", link.target_slug, 
                        match link.link_type {
                            khimoo_portfolio::LinkType::WikiLink => "wiki",
                            khimoo_portfolio::LinkType::MarkdownLink => "markdown",
                        }
                    );
                }
            }
            if !metadata.tags.is_empty() {
                println!("   üè∑Ô∏è  Tags: {:?}", metadata.tags);
            }
        }

        Ok(ProcessedArticle {
            slug,
            title: metadata.title.clone(),
            content: markdown_content,
            metadata,
            file_path: file_path.to_string_lossy().to_string(),
            outbound_links,
            inbound_count: 0, // Will be calculated later
            processed_at: Utc::now().to_rfc3339(),
        })
    }

    fn generate_slug(&self, file_path: &Path) -> String {
        file_path
            .file_stem()
            .unwrap()
            .to_string_lossy()
            .to_string()
            .to_lowercase()
            .replace(' ', "-")
    }

    fn calculate_inbound_counts(&self, mut articles: Vec<ProcessedArticle>) -> Result<Vec<ProcessedArticle>> {
        // Create a map of slug -> index for quick lookup
        let slug_to_index: HashMap<String, usize> = articles
            .iter()
            .enumerate()
            .map(|(i, article)| (article.slug.clone(), i))
            .collect();

        // Count inbound links by collecting them first
        let mut inbound_counts: HashMap<String, usize> = HashMap::new();
        
        for article in &articles {
            for link in &article.outbound_links {
                if slug_to_index.contains_key(&link.target_slug) {
                    *inbound_counts.entry(link.target_slug.clone()).or_insert(0) += 1;
                }
            }
        }

        // Apply the counts
        for article in &mut articles {
            article.inbound_count = inbound_counts.get(&article.slug).copied().unwrap_or(0);
        }

        Ok(articles)
    }

    fn build_link_graph(&self, articles: &[ProcessedArticle]) -> Result<LinkGraphData> {
        let mut graph = HashMap::new();
        let article_slugs: std::collections::HashSet<_> = 
            articles.iter().map(|a| &a.slug).collect();
        
        // First pass: create nodes and direct connections
        for article in articles {
            let mut connections = Vec::new();
            
            // Process outbound links (direct links only)
            for link in &article.outbound_links {
                if article_slugs.contains(&link.target_slug) {
                    connections.push(GraphConnection {
                        target: link.target_slug.clone(),
                        connection_type: ConnectionType::DirectLink,
                        bidirectional: false,
                    });
                }
            }
            
            graph.insert(article.slug.clone(), GraphNode {
                connections,
                inbound_count: article.inbound_count,
            });
        }
        
        // Second pass: detect bidirectional links
        let mut bidirectional_pairs = std::collections::HashSet::new();
        
        for (source_slug, source_node) in &graph {
            for connection in &source_node.connections {
                let target_slug = &connection.target;
                
                // Check if target also links back to source
                if let Some(target_node) = graph.get(target_slug) {
                    let has_backlink = target_node.connections
                        .iter()
                        .any(|c| c.target == *source_slug);
                    
                    if has_backlink {
                        // Create a canonical pair (smaller slug first) to avoid duplicates
                        let pair = if source_slug < target_slug {
                            (source_slug.clone(), target_slug.clone())
                        } else {
                            (target_slug.clone(), source_slug.clone())
                        };
                        bidirectional_pairs.insert(pair);
                    }
                }
            }
        }
        
        // Third pass: update bidirectional connections
        for (slug1, slug2) in &bidirectional_pairs {
            // Update connections in both nodes
            if let Some(node1) = graph.get_mut(slug1) {
                for connection in &mut node1.connections {
                    if connection.target == *slug2 {
                        connection.connection_type = ConnectionType::Bidirectional;
                        connection.bidirectional = true;
                    }
                }
            }
            
            if let Some(node2) = graph.get_mut(slug2) {
                for connection in &mut node2.connections {
                    if connection.target == *slug1 {
                        connection.connection_type = ConnectionType::Bidirectional;
                        connection.bidirectional = true;
                    }
                }
            }
        }
        
        let total_connections = graph.values()
            .map(|node| node.connections.len())
            .sum();
        
        if self.verbose {
            println!("üï∏Ô∏è  Built link graph with {} nodes and {} connections", 
                graph.len(), total_connections);
            println!("   üîó Bidirectional pairs: {}", bidirectional_pairs.len());
        }
        
        Ok(LinkGraphData {
            graph,
            generated_at: Utc::now().to_rfc3339(),
            total_connections,
        })
    }

    fn write_articles_data(&self, articles: &[ProcessedArticle]) -> Result<()> {
        let articles_data = ArticlesData {
            articles: articles.to_vec(),
            generated_at: Utc::now().to_rfc3339(),
            total_count: articles.len(),
            home_articles: articles
                .iter()
                .filter(|a| a.metadata.home_display)
                .map(|a| a.slug.clone())
                .collect(),
        };
        
        let output_path = self.output_dir.join("articles.json");
        let json = serde_json::to_string_pretty(&articles_data)
            .context("Failed to serialize articles data")?;
        
        std::fs::write(&output_path, json)
            .with_context(|| format!("Failed to write articles data to {:?}", output_path))?;
        
        if self.verbose {
            println!("üìÑ Written articles data to: {:?}", output_path);
        }
        
        Ok(())
    }

    fn write_link_graph_data(&self, link_graph: &LinkGraphData) -> Result<()> {
        let output_path = self.output_dir.join("link-graph.json");
        let json = serde_json::to_string_pretty(link_graph)
            .context("Failed to serialize link graph data")?;
        
        std::fs::write(&output_path, json)
            .with_context(|| format!("Failed to write link graph data to {:?}", output_path))?;
        
        if self.verbose {
            println!("üï∏Ô∏è  Written link graph data to: {:?}", output_path);
        }
        
        Ok(())
    }

    fn display_summary(&self, articles: &[ProcessedArticle], link_graph: &LinkGraphData) {
        println!("\nüìä Processing Summary:");
        println!("   üìö Total articles: {}", articles.len());
        
        let home_articles = articles.iter().filter(|a| a.metadata.home_display).count();
        println!("   üè† Home display articles: {}", home_articles);
        
        let total_links: usize = articles.iter().map(|a| a.outbound_links.len()).sum();
        println!("   üîó Total outbound links: {}", total_links);
        
        let articles_with_inbound: usize = articles.iter().filter(|a| a.inbound_count > 0).count();
        println!("   üì• Articles with inbound links: {}", articles_with_inbound);
        
        // Link graph statistics
        println!("   üï∏Ô∏è  Link graph connections: {}", link_graph.total_connections);
        let bidirectional_count = link_graph.graph.values()
            .flat_map(|node| &node.connections)
            .filter(|conn| conn.bidirectional)
            .count() / 2; // Divide by 2 since bidirectional links are counted twice
        println!("   ‚ÜîÔ∏è  Bidirectional connections: {}", bidirectional_count);
        
        // Category breakdown
        let mut categories: HashMap<String, usize> = HashMap::new();
        for article in articles {
            let category = article.metadata.category.as_deref().unwrap_or("uncategorized");
            *categories.entry(category.to_string()).or_insert(0) += 1;
        }
        
        if !categories.is_empty() {
            println!("   üìÇ Categories:");
            for (category, count) in categories {
                println!("      {}: {}", category, count);
            }
        }
        
        // Tag statistics
        let mut all_tags: HashMap<String, usize> = HashMap::new();
        for article in articles {
            for tag in &article.metadata.tags {
                *all_tags.entry(tag.clone()).or_insert(0) += 1;
            }
        }
        
        if !all_tags.is_empty() {
            println!("   üè∑Ô∏è  Most common tags:");
            let mut tag_counts: Vec<_> = all_tags.into_iter().collect();
            tag_counts.sort_by(|a, b| b.1.cmp(&a.1));
            for (tag, count) in tag_counts.into_iter().take(5) {
                println!("      {}: {}", tag, count);
            }
        }
        
        // Link validation preview
        let existing_slugs: std::collections::HashSet<_> = articles.iter().map(|a| &a.slug).collect();
        let mut broken_links = 0;
        
        for article in articles {
            for link in &article.outbound_links {
                if !existing_slugs.contains(&link.target_slug) {
                    broken_links += 1;
                }
            }
        }
        
        if broken_links > 0 {
            println!("   ‚ö†Ô∏è  Broken links detected: {} (run validate-links for details)", broken_links);
        } else {
            println!("   ‚úÖ All links valid");
        }
    }
}